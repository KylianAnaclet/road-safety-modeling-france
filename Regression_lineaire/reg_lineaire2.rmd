---
title: "Analyse des Facteurs de Gravité des Accidents Routiers (ONISR 2021)"
subtitle: "Régression linéaire"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4)
```

```{r}
# Chargement des librairies utiles
if(!require(dplyr)) install.packages("dplyr")
if(!require(stringr)) install.packages("stringr")
if(!require(car)) install.packages("car")
if(!require(ggplot2)) install.packages("ggplot2")
library(dplyr)
library(stringr)
library(car)
library(ggplot2)
```

# Introduction

L'objectif est de déterminer les facteurs qui ont le plus d'impact sur la sévérité d'un accident de la route en France en 2021. Nous cherchons donc à expliquer un **Score de Sévérité** à partir de variables issues de la base de données ONISR 2021 (Observatoire National Interministériel de la Sécurité Routière).




# Préparation des Données


## Importation et Nettoyage

```{r}
df <- read.csv("ONISR-2021.csv")
head(df)
```

Pour le traitement des données, nous allons commencer par définir un **Score de Sévérité** pondéré par le nombre de tué, hospitalisé et blessé. Cette pondération est basé sur l'échelle AIS (Abbreviated Injury Scale); 1 pour une blessure légère, 3 pour une blessure sérieuse et 6 pour une blessure maximale.

```{r}
# On ajoute ce score de sévérité dans notre df
df <- df %>%
  mutate(
    Score_Severite = 1*nb_blesse + 3*nb_hospitalise + 6*nb_tue,
  )

colnames(df)
```


```{r}
# Histogramme du score de sévérité

df %>%
  # Séparation du score en fonction de la gravité
  mutate(Tranche_Gravite = case_when(
    Score_Severite <= 2 ~ "1. Léger (0-2)",       
    Score_Severite <= 4 ~ "2. Intermédiaire (3-4)", 
    Score_Severite >= 5 ~ "3. Grave (5+)"                        
  )) %>%
  
  
  ggplot(aes(x = Score_Severite)) +
  
  # On utilise la nouvelle colonne 'Tranche_Gravite' pour la couleur (fill)
  geom_bar(aes(fill = Tranche_Gravite), 
           width = 1, 
           color = "black", 
           alpha = 0.9) +

  scale_fill_manual(values = c(
    "1. Léger (0-2)" = "#69b3a2",        
    "2. Intermédiaire (3-4)" = "#F39C12", 
    "3. Grave (5+)" = "#C0392B"          
  )) +
  
  # Axe des x gradués de 1 par 1 jusqu'à 10
  scale_x_continuous(breaks = seq(0, 10, by = 1)) +
  coord_cartesian(xlim = c(0, 10)) +
  
  labs(title = "Distribution du Score de Sévérité",
       x = "Score de Sévérité", 
       y = "Nombre d'accidents",
       fill = "Niveau de gravité") + 
  
  theme_minimal() +
  theme(plot.title = element_text(size = 20, hjust = 0.5),
        legend.position = "top") 
```
```{r}
summary(df$Score_Severite)
```


```{r}
str(df)
```
On observe qu'un certain nombre de variables ne nous sera pas utile pour notre modèle de régression pour différentes raisons. En effet, les variables qui servent d'identifiants, celles ayant trop de catégories ou encore les variables avec une grande proportion de valeurs manquantes. 
Pour les variables ayant trop de modalités, il est préférable de regrouper les différentes modalités car juste supprimer ces variables pourrait faire perdre beacoup d'informations pour notre modèle.

```{r}
# On crée donc une fonction permettant de regrouper les départements (+100) en régions (13 métropolitaine + 1 pour l'Outre-Mer)

get_region <- function(dep_code) {
  # On s'assure que l'entrée est un chr
  d <- as.character(dep_code)
  
  case_when(
    # Gestion de la Corse (2A et 2B)
    d %in% c("2A", "2B", "20") ~ "Corse",
    
    # Gestion des DOM (codes commençant par 97)
    substr(d, 1, 2) == "97" ~ "Outre-Mer",
    
    # Départements standards (on gère les cas "1" et "01")
    # Auvergne-Rhône-Alpes
    d %in% c("01", "1", "03", "3", "07", "7", "15", "26", "38", "42", "43", "63", "69", "73", "74") ~ "Auvergne-Rhône-Alpes",
    
    # Hauts-de-France
    d %in% c("02", "2", "59", "60", "62", "80") ~ "Hauts-de-France",
    
    # PACA
    d %in% c("04", "4", "05", "5", "06", "6", "13", "83", "84") ~ "PACA",
    
    # Grand Est
    d %in% c("08", "8", "10", "51", "52", "54", "55", "57", "67", "68", "88") ~ "Grand Est",
    
    # Occitanie
    d %in% c("09", "9", "11", "12", "30", "31", "32", "34", "46", "48", "65", "66", "81", "82") ~ "Occitanie",
    
    # Normandie
    d %in% c("14", "27", "50", "61", "76") ~ "Normandie",
    
    # Nouvelle-Aquitaine
    d %in% c("16", "17", "19", "23", "24", "33", "40", "47", "64", "79", "86", "87") ~ "Nouvelle-Aquitaine",
    
    # Centre-Val de Loire
    d %in% c("18", "28", "36", "37", "41", "45") ~ "Centre-Val de Loire",
    
    # Bourgogne-Franche-Comté
    d %in% c("21", "25", "39", "58", "70", "71", "89", "90") ~ "Bourgogne-Franche-Comté",
    
    # Bretagne
    d %in% c("22", "29", "35", "56") ~ "Bretagne",
    
    # Pays de la Loire
    d %in% c("44", "49", "53", "72", "85") ~ "Pays de la Loire",
    
    # Île-de-France
    d %in% c("75", "77", "78", "91", "92", "93", "94", "95") ~ "Île-de-France"
  )
}
```


```{r}
df <- df %>%
  
  # On transforme l'heure en un numeric compris entre 0 et 23
  # en extrayant la première partie de hrmn (heure:min)
  mutate(
    Heure_Num= as.numeric(str_sub(hrmn, 1, 2))
  ) %>%
  
  # On crée une variable Moment pour séparer les moments de la journée
  mutate(
    Moment = case_when(
      Heure_Num >= 0 & Heure_Num <= 5  ~ "Nuit Profonde",
      Heure_Num >= 6 & Heure_Num <= 10 ~ "Matin (Pointe)",
      Heure_Num >= 11 & Heure_Num <= 16 ~ "Journee (Creuse)",
      Heure_Num >= 17 & Heure_Num <= 21 ~ "Soir (Pointe)",
      Heure_Num >= 22 & Heure_Num <= 23 ~ "Soiree",
      TRUE ~ "Autre" # Sécurité
    ),
    # On transforme en factor et on choisit "Journée" comme référence
    Moment = factor(Moment, levels = c("Journee (Creuse)", "Matin (Pointe)", "Soir (Pointe)", "Soiree", "Nuit Profonde")))%>%
  
  # On crée la colonne région
  mutate(
    Region = get_region(dep),
    Region = as.factor(Region)
  ) %>%
  
  # On garde que les variables intéressantes i.e
  # en enlevant les identifiants (Num_Acc, adr), les variables quasi vides (pr, lartpc..), ou avec trop de catégories (com, dep)
  select(
    Score_Severite,
    Region,
    Moment,
    lum, agg, int, atm, col, 
    catr, circ, prof, plan, surf, infra, situ, 
    vma, 
    nb_velo, nb_moto, nb_voiture, nb_utilitaire, nb_camion, nb_transport
  ) %>%
  
    
  # On convertit les variables chr en factor pour que r les traitent comme des variables qualitatives
  mutate_if(is.character, as.factor) %>%
  
  # On supprime les lignes avec des valeurs manquantes
  na.omit
```


```{r}
str(df)
```
Après ce nettoyage, on a perdu seulement environ 4 000 lignes, ce qui est négligeable puisqu'il nous reste 52 000 lignes et nous avons gagné en qualité.

```{r}
# R crée des levels pour les variables factor afin de comparer ces levels par rapport à un level de référence (Intercept).
# Mais initialement R choisit le level de référence par ordre alphabétique, ce qui n'est pas forcément souhaitable 
```

```{r}
# Par exemple, R a choisit la référence "aube ou crépuscule" alors qu'on voudrait mieux comparer par rapport à la référence "plein jour"
levels(df_final$lum)
```

```{r}
# On affecte le level de référence que l'on souhaite
df_final$lum <- relevel(df_final$lum, ref = "plein jour", silent=TRUE)
df_final$atm <- relevel(df_final$atm, ref = "normale", silent=TRUE)
df_final$prof <- relevel(df_final$prof, ref = "plat", silent=TRUE)
df_final$plan <- relevel(df_final$plan, ref = "rectiligne", silent=TRUE)
df_final$surf <- relevel(df_final$surf, ref = "normale", silent=TRUE)
df_final$int <- relevel(df_final$int, ref = "aucune", silent=TRUE)
df_final$catr <- relevel(df_final$catr, ref = "departementale", silent=TRUE)
df_final$situ <- relevel(df_final$situ, ref = "chaussee", silent=TRUE)
```

```{r}
# Vérifions 
cat("Référence pour LUM :", levels(df_final$lum)[1], "\n")
cat("Référence pour ATM :", levels(df_final$atm)[1], "\n")
cat("Référence pour PROF :", levels(df_final$prof)[1], "\n")
cat("Référence pour PLAN :", levels(df_final$plan)[1], "\n")
cat("Référence pour SURF :", levels(df_final$surf)[1], "\n")
cat("Référence pour INT :", levels(df_final$int)[1], "\n")
cat("Référence pour CATR :", levels(df_final$catr)[1], "\n")
cat("Référence pour SITU :", levels(df_final$situ)[1], "\n")
```

Ainsi, lorsque l'on va calculer la régression, Intercept correspondra au score de sévérité moyen quand tout est à la référence (Plein jour, Météo normale, route plate, etc.)

# Modèle de Régression Linéaire

```{r}
# Modèle de régression: Score de sévérité expliqué par toutes les colonnes
modele <- lm(Score_Severite ~ ., data = df)

summary(modele)
```
```{r}


# On affiche les 4 graphiques de diagnostic
plot(modele)


```
1) Les résidus sont globalement centré autour de 0 mais on observe une augmentation de la dispersion des résidus pour les valeurs élevées.
2) Les points suivent approximativement la droite normale théorique mais pour les valeurs élevées, les points s'écartent de la droite. Cela indique que les résidus ne suivent pas une distribution normale.
3) La variabilité des résidus augmente avec les valeurs prédites par le modèle (courbe rouge croissante)
4) On identifie un point extrême tout à droite
```{r}
# Distribution des résidus
hist(resid(modele), 
     breaks = 50, 
     col = "lightblue", 
     main = "Distribution des Résidus",
     xlab = "Erreur du modèle")
```
On peut observer un pic autour de 0 et une asymétrie positive, qui confirme visuellement la non-normalité des résidus.

```{r}
# Calcul du VIF
vif_values <- vif(modele)


print(vif_values)
```

```{r}
# Modèle de régression avec le log: Score de sévérité expliqué par toutes les colonnes
modele2 <- lm(log(Score_Severite) ~ ., data = df)

summary(modele2)
```

```{r}
plot(modele2)
```
Les points suivent désormais presque parfaitement la droite normale. Cela signifie que l'hypothèse de normalité des résidus est respectée.

```{r}
hist(resid(modele2), 
     breaks = 20, 
     col = "lightblue", 
     main = "Distribution des Résidus",
     xlab = "Erreur du modèle")
```
Graphique un peu plus en forme de cloche qui fait penser à la loi Normale.


On enlève les variables surf, prof et circ de notre modèle car ces variables ne sont statistiquement pas significatives sur le score de sévérité.
```{r}
modele3 <- lm(log(Score_Severite) ~. - surf - prof-circ, data=df)
summary(modele3)
```
```{r}
max(coef(modele3))
```

```{r}
library(ggplot2)
library(dplyr)


# Standardiser les variables pour comparer l'importance
df_scaled <- df %>%
  mutate(across(where(is.numeric), scale))

model_scaled <- lm(Score_Severite ~ .-surf-prof-circ, data = df_scaled)

# 2. Créer un dataframe des coefficients
coef_df <- data.frame(
  Variable = names(coef(model_scaled))[-1],  # Exclure l'intercept
  Coefficient = coef(model_scaled)[-1] 
) %>%
  arrange(desc(Coefficient)) %>%
  head(5)  # Garder les 5 plus importantes

# 3. Graphique en barres
ggplot(coef_df, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_text(aes(label = round(Coefficient, 3)), 
            hjust = -0.2, size = 4, fontface = "bold") +
  

  
  coord_flip() +  
  
  labs(
    title = "Les variables les plus influentes sur le Score de Sévérité",
    x = "Variable",
    y = "Coefficient"
  ) +
  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.major.y = element_blank(),
    legend.position = "top",
    plot.background = element_rect(fill = "white", color = NA)
  ) +
  
  ylim(0, max(coef_df$Coefficient) * 1.15)  
```

