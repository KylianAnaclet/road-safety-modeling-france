\documentclass[a4paper, 11pt]{article}

% --- PACKAGES ESSENTIELS ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel} % Pour la typo française
\usepackage{geometry}
\geometry{hmargin=2cm, vmargin=2cm} % Marges optimisées pour gagner de la place
\usepackage{graphicx} % Pour les images
\usepackage{booktabs} % Pour des tableaux "pros"
\usepackage{amsmath}  % Pour les maths
\usepackage{float}    % Pour forcer le placement des images [H]
\usepackage{xcolor}   % Pour mettre en valeur certains mots

\begin{document}

\thispagestyle{empty} % Pas de numéro de page sur la 1ère (optionnel)

% =============================================================================
\section{Analyse sur la fréquence des accidents}
% =============================================================================

\section{Une double approche de la modélisation}

Dans cette étude, nous analysons la variabilité du nombre d'accidents corporels par unité d'exposition afin d'identifier des facteurs de risque structurels, en comparant deux modèles de fréquences issus des données de l'ONISR, différant par l'échelle géographique et le facteur d'exposition utilisé.

\begin{itemize}
    \item \textbf{Approche généraliste} (\textit{échelle départementale – France entière}) : le facteur d’exposition est la population départementale, issue des données de l’INSEE (dataset \texttt{Df\_Pop\_brut}).\\
    Cette approche permet d’analyser un risque d’accident rapporté à la population, relevant d’une lecture de type \textit{santé publique}.\\
    Cela correspond au fichier \texttt{Frequence.Rmd}.
    
    \item \textbf{Approche de précision} (\textit{échelle communale – région Auvergne-Rhône-Alpes}) : le facteur d’exposition est le Trafic Moyen Journalier Annuel (TMJA), issu des comptages routiers (dataset \texttt{Df\_TMJA\_brut}).\\
    Cette seconde approche vise à estimer un risque rapporté à l’usage réel de l’infrastructure routière, offrant une lecture plus fine du risque routier.\\
    Cela correspond au fichier \texttt{Frequence\_TMJA.Rmd}.
\end{itemize}
À l’issue de ces deux approches, les données sont structurées sous la forme d’un jeu de données final (\texttt{df\_final}), regroupant les informations issues de l’ONISR ainsi que le nombre d’accidents normalisé par le facteur d’exposition retenu (population ou TMJA selon le modèle), et servant de base à la modélisation statistique.
\section{Méthodologie : de Poisson à la Binomiale Négative}
\subsection{Modèles linéaires généralisés et variables explicatives}
Pour modéliser la variable de comptage $Y$ (nombre d'accidents), nous utilisons le cadre des Modèles Linéaires Généralisés (GLM). Ce type de modèle relie l'espérance de la variable cible $E(Y)$ à une combinaison linéaire des variables explicatives $X_i$ via une fonction de lien logarithmique.

L'équation générale s'écrit :
\begin{equation}
    \log(E(Y)) = \beta_0 + \sum_{i=1}^{p} \beta_i X_i + \text{offset}
\end{equation}

\textbf{Le rôle de l'offset :}
L'offset est un terme dont le coefficient est contraint à 1. Il est fondamental ici car il permet de passer d'une modélisation en "nombre brut" à une modélisation en "taux". Ainsi,
selon l'approche, l'exposition est soit la Population ($\log(\text{Pop})$), soit le Trafic ($\log(\text{TMJA})$). 
\subsection{Justification du modèle : De Poisson à la Binomiale Négative}

Dans un premier temps, nous avons modélisé la fréquence des accidents par une régression de **Poisson**. L'évaluation de la pertinence de ce modèle repose sur deux indicateurs statistiques extraits des résultats (\texttt{summary}) :

\begin{itemize}
    \item \textbf{La Déviance :} Elle quantifie l'écart entre les prédictions du modèle et les observations réelles (l'ajustement global).
    \item \textbf{L'AIC (Akaike Information Criterion) :} Il s'agit d'un critère de comparaison permettant de sélectionner le modèle offrant le meilleur compromis entre précision et complexité (plus l'AIC est faible, meilleur est le modèle).
\end{itemize}

\paragraph{Limites de Poisson et Surdispersion}
La loi de Poisson repose sur une hypothèse restrictive d'équidispersion, imposant que la variance soit égale à l'espérance ($Var(Y) = E(Y)$). Or, nos données présentent une variance nettement supérieure à la moyenne (surdispersion), ce qui suggère que des facteurs hétérogènes influencent fortement l'accidentologie.
L'utilisation stricte de Poisson risquant de fausser l'estimation des erreurs types (sous-estimation du risque d'erreur), nous avons opté pour la **Loi Binomiale Négative**. Ce modèle généralise celui de Poisson en ajoutant un paramètre de dispersion $\theta$, permettant de capter correctement cette variabilité excessive. \\
	A présent, voyons les résultats appliquées cette méthode pour nos deux approches : généraliste et précision
\section{Résultats et Interprétation}
\subsection{Approche Généraliste (Population) : La preuve par la Déviance}

Dans cette première approche, nous avons ajusté un GLM sur l'ensemble des départements en utilisant la population comme offset. Le modèle s'écrit :
\begin{equation*}
    \texttt{glm(Nb\_Accidents} \sim \text{Pluie} + \text{Autoroute} + \dots + \text{offset}(\log(\text{Population}))
\end{equation*}

\paragraph{Échec du modèle de Poisson}
Les résultats obtenus avec la loi de Poisson révèlent une inadéquation majeure avec les données. La **déviance résiduelle** atteint une valeur critique d'environ \textbf{10 000}, bien supérieure au nombre de degrés de liberté.
Le graphique "Prédiction vs Réalité" montre cet échec.En effet, il faut savoir que tout de même le modelè Poisson réussit à expliquer les accidents dans certains départements mais il échoue notamment dans les métropoles notamment le 75 (Paris), 59 (Lille), 13 (Marseille),... Ainsi, on a décidé de montrer que cela s'explique potentiellement par le fait de la surdispersion comme le témoigne la figure ci-dessous,

\begin{figure}[H]
    \centering
    % ICI C'EST TON FICHIER :
    \includegraphics[width=0.85\textwidth]{figure.png}
    \caption{Diagnostic des Résidus : Mise en évidence de la structure en entonnoir}
    \label{fig:surdispersion}
\end{figure}

En effet, on constate notamment que la majorité des points noirs ne sont pas dans la zone souhaitée (entre les deux lignes rouges). Ainsi, cela explique que la variance augmente avec la moyenne d'où le fait qu'on a décidé d'opter pour la méthode Binomiale Négative.

\paragraph{Apport de la Binomiale Négative}
Le passage à la loi Binomiale Négative permet de corriger ce biais structurel. En introduisant un paramètre de dispersion $\theta$, la déviance chute spectaculairement à environ \textbf{4 000}.
Cette réduction de plus de 60\% de la déviance confirme que la loi Binomiale Négative est la seule adaptée pour modéliser l'hétérogénéité des données d'accidents à cette échelle.

% =============================================================================
\subsection{Approche de Précision (TMJA) : Résultats et Interprétation}
% =============================================================================

Nous appliquons ici la même méthodologie (GLM adapté), en restreignant cette fois-ci l'analyse à la région Auvergne-Rhône-Alpes et en définissant l'offset par le logarithme du trafic moyen ($\log(\text{TMJA})$).

L'intégration de cet offset et le passage à la loi Binomiale Négative améliorent drastiquement la qualité du modèle (l'AIC chute de 11 887 à \textbf{3 969}).
L'analyse des coefficients met en évidence une dualité marquée :

\begin{itemize}
    \item \textbf{La Nuit (1.64) :} C'est un facteur aggravant majeur qui augmente le risque de 64\% à trafic constant (visibilité réduite, fatigue).
    \item \textbf{La Neige (0.61) et les Virages (0.23) :} Ces variables présentent des coefficients inférieurs à 1, ce qui semble contre-intuitif (effet "protecteur").
\end{itemize}

Ce paradoxe apparent s'explique par une forte \textbf{adaptation comportementale}. Face à des conditions visiblement difficiles (routes de montagne, intempéries), les conducteurs — souvent habitués à ces contextes locaux — augmentent leur vigilance et réduisent leur vitesse. À l'inverse, les environnements jugés "sûrs" (lignes droites, temps sec) favorisent le relâchement de l'attention et les vitesses élevées.

% =============================================================================
\section{Limites et Perspectives}
% =============================================================================
Malgré la robustesse statistique de l'approche par la loi Binomiale Négative, cette étude présente certaines limites inhérentes à la nature des données et aux métriques utilisées.

\begin{itemize}
    \item \textbf{Biais d'agrégation :} Le regroupement des accidents par commune induit un lissage de l'information qui risque de masquer des "points noirs" très localisés (carrefours spécifiques), noyés dans la moyenne globale de la ville.
    
    \item \textbf{Nature statique du TMJA :} Le trafic moyen annuel est un indicateur figé qui ne capture pas la réalité dynamique (congestion, heure de pointe). Or, le risque d'accident diffère radicalement entre une circulation fluide et un trafic saturé.
    
    \item \textbf{Facteur humain inobservé :} Une part de la variance reste inexpliquée car nos modèles structurels ne contiennent aucune donnée sur l'état du conducteur au moment du choc (fatigue, alcoolémie, distraction), pourtant déterminant majeur de l'accidentologie.
\end{itemize}

\end{document}
\end{document}

