---
title: "Clustering K-prototypes"
subtitle: "Équipe 19 – ANACLET Kylian, BIN DAIVIN Muhamad Zaiinizee, CHENNOUF Younes, KEBAIRI Ilyes"
output:
  pdf_document: default
  html_document: default
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Le jeu de données contient à la fois des variables numériques et catégorielles (région, condition d'éclairage, condition atmosphérique, etc.). L'algorithme K-means classique n'est donc pas adapté, car il repose sur la distance euclidienne, qui n'a pas de sens pour les variables catégorielles, même après encodage one-hot.

Il est préférable d'utiliser **K-prototypes**, qui combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité spécifique pour les variables catégorielles. Cet algorithme permet ainsi d'obtenir des clusters plus fiables et cohérents.

# K-prototypes

L'algorithme K-prototypes implémente la méthode de partitionnement par minimisation de la distance entre les observations et les centres de clusters. Plus précisément, il cherche à minimiser la fonction suivante :

$$
\min_{c_1,...,c_k} \sum_{k=1}^{K} \sum_{i,G(i)=k} d(x^{(i)}, c_k)
$$

où $d(x^{(i)}, c_k)$ représente la distance entre l'observation $x^{(i)}$ et le centre du cluster $c_k$. Cette distance combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité pour les variables catégorielles.

La minimisation est réalisée automatiquement par la fonction `kproto()` à travers un processus itératif. L'algorithme affecte chaque observation au cluster dont le centre est le plus proche, puis recalcule les centres, et répète ces étapes jusqu'à convergence. Le résultat de cette minimisation est observable via `tot.withinss`, qui représente la somme totale des distances intra-clusters.

## A. Chargement et préparation des données

Le jeu de données final étant **très volumineux**, un échantillon aléatoire a été extrait pour faciliter le traitement et le clustering.

```{r}
if(!require(dplyr)) install.packages("dplyr")
library(dplyr)

set.seed(321)
n_sample <- 5000

# Exécuter le script « nettoyage_donnees_clustering.Rmd », situé dans le dossier clustering, afin de générer le fichier « donnees_clustering.csv ».
df <- read.csv("../data/donnees_clustering.csv")
df_complete <- df %>% 
  mutate(across(where(is.character), as.factor)) %>%
  select(agg, lum, atm, col, vma)

# Convertir toutes les variables de type character en factor
# Puis sélectionner uniquement les 5 variables retenues pour le clustering
df <- df %>% 
  mutate(across(where(is.character), as.factor)) %>%
  select(agg, lum, atm, col, vma)

# On tire par hasard 1000 accidents
df <- df[sample(nrow(df), n_sample), ]
```

On sélectionne ces 5 variables (agg, lum, atm, col, vma) car elles capturent les conditions principales de l’accident (localisation, luminosité, météo, type de collision et vitesse autorisée) et donnent les meilleurs résultats de clustering.

```{r}
str(df)
```

## B. Verification de la légitimité de clustering

Avant de commencer on doit tout d'abord s'assurer que l'utilisation de la méthode de clustering est légitime ici (s'assurer que les données ne sont pas réparties de façon homogène). Pour ce faire on utilise la méthode statistique en calculant la statistique de Hopkins calculé avec la formule :

$$
H = \frac{\sum_{i=1}^{n}{x_i}}{\sum_{i=1}^{n}{x_i}+\sum_{i=1}^{n}{y_i}}
$$

où :

-   $x_i$ la distance d'un i-éme point $p_i$ choisi aléatoirement dans notre distribution à tster avec son plus proche voisin dans la même distribution

-   $y_i$ **:** la distance d'un i-éme point qi choisi aléatoirement dans une distribution aléatoire uniforme avec son plus proche voisin dans la distribution

Nos hypothèses étant :

-   $H_0$ **:** Les données sont uniformément distribués

-   $H_1$ **:** Les données ne sont pas uniformément distribués

    ```{r}
    if(!require(hopkins)) install.packages("hopkins")
    library(hopkins)

    numeric_cols <- sapply(df, is.numeric)
    n_numeric <- sum(numeric_cols)

    if (n_numeric >= 2) {
      h_stat <- hopkins(df[, numeric_cols])
      cat("Statistique de Hopkins :", round(h_stat, 4), "\n")
    } else {
      cat("Hopkins non calculable : seulement", n_numeric, "variable numérique (vma).\n")
      cat("   (La fonction nécessite au moins 2 variables numériques)\n")
    }
    ```

## C. Choix du nombre de clusters (k)

On applique l'algorithme k-prototypes pour différentes valeurs de k.

```{r}
set.seed(321)
if(!require(clustMixType)) install.packages("clustMixType")
if(!require(cluster)) install.packages("cluster")
library(clustMixType)
library(cluster)

k_max <- 10
cost <- numeric(k_max)
sil_avg <- numeric(k_max)

# Calculer la distance de Gower sur les données non normalisées
d <- daisy(df, metric = "gower")

# Exécuter k-prototypes pour différentes valeurs de k
for (k in 2:k_max) {
  kp <- kproto(df, k, verbose = FALSE)
  cost[k] <- kp$tot.withinss
  
  sil <- silhouette(kp$cluster, d)
  sil_avg[k] <- mean(sil[, 3])
}

# Plot méthode du coude
plot(2:k_max, cost[2:k_max], 
     main = "Méthode du coude pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Somme des distances")

# Plot méthode de la silhouette
plot(2:k_max, sil_avg[2:k_max], 
     main = "Méthode de la silhouette pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Silhouette moyenne")
```

### Observations

La méthode du coude montre une stabilisation à partir de k = 4, tandis que l’indice de Silhouette est maximal à k = 2.

Pour choisir le meilleur k, nous testons k = 2, 3 et 4 en comparant leurs indices de qualité.

## D. Application de k-prototypes et analyse des résultats

```{r}
set.seed(321)
res_k2 <- kproto(df, k = 2, verbose = FALSE)
res_k3 <- kproto(df, k = 3, verbose = FALSE)
res_k4 <- kproto(df, k = 4, verbose = FALSE)
```

## E. Test de la qualité du clustering

Afin d'évaluer la qualité du clustering on évalue les deux indices :

-   l'indice de Silhouette pour chaque cluster qui pour chaque élément évalue à quel point un élément est "similaire" par rapport aux autres éléments du même cluster .

-   l'indice de Dunn qui est défini comme suit :

    $$
    D=\frac{min.separation}{max.diameter}
    $$

    De plus on veille à adapter cette methode d'évaluation à la methode de clustering utilisée (k-prototype) et ce en utilisant justement la distance de gower qui prend en compte les variables catégorielles

```{r}
if(!require(fpc)) install.packages("fpc")
library(cluster)
library(fpc)

dist_gower <- daisy(df, metric = "gower")

evaluer_k <- function(res, k_val) {
  cat("=== k =", k_val, "===\n")
  sil <- silhouette(res$cluster, dist_gower)
  cat("Silhouette moyenne :", round(mean(sil[,3]), 4), "\n")
  cat("Détail :", paste(round(summary(sil)$clus.avg.widths, 4), collapse = ", "), "\n")
  dunn <- cluster.stats(dist_gower, res$cluster)$dunn
  cat("Dunn :", round(dunn, 6), "\n\n")
}

evaluer_k(res_k2, 2)
evaluer_k(res_k3, 3)
evaluer_k(res_k4, 4)
```

Bien que $k=2$ offre la meilleure qualité statistique avec des clusters très cohérents, une partition en seulement deux groupes est trop simpliste pour refléter la diversité des accidents routiers. Nous retenons donc $k=3$, qui propose trois profils plus nuancés et interprétables, malgré une qualité statistique légèrement moindre.

#### Distribution des observations par cluster

```{r}
table(res_k3$cluster)
```

## F. Comparaison échantillon vs données complètes

### Moyennes des variables numériques par cluster

```{r}
library(clustMixType)

# Moyennes vma
cat("Moyenne vma sur l'échantillon :\n")
print(round(tapply(df$vma, res_k3$cluster, mean), 1))

res_full <- kproto(df_complete, k = 3, verbose = FALSE)
cat("\nMoyenne vma sur les données complètes :\n")
print(round(tapply(df_complete$vma, res_full$cluster, mean), 1))
```

### **Répartition des variables catégorielles par cluster**

```{r}
cat_cols <- sapply(df, is.factor)

for (col in names(df)[cat_cols]) {
  cat("\n=== Variable :", col, "===\n\n")
  
  cat("Échantillon :\n")
  print(table(df[[col]], res_k3$cluster))
  
  cat("\nDonnées complètes :\n")
  print(table(df_complete[[col]], res_full$cluster))
  
  cat("\n")
}
```

## G. Visualisation des clusters k-prototypes

```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(scales)  # Pour percent()

df_vis <- df %>% mutate(cluster = as.factor(res_k3$cluster))

p1 <- ggplot(df_vis, aes(x = cluster, y = vma, fill = cluster)) +
  geom_boxplot() + theme_minimal() + theme(legend.position = "none") +
  labs(title = "Vitesse maximale autorisée", x = "Cluster", y = "vma (km/h)")

p2 <- ggplot(df_vis, aes(x = cluster, fill = agg)) + geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) + theme_minimal() +
  labs(title = "Localisation", x = "Cluster", y = "Proportion")

p3 <- ggplot(df_vis, aes(x = cluster, fill = lum)) + geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) + theme_minimal() +
  labs(title = "Luminosité", x = "Cluster", y = "Proportion")

p4 <- ggplot(df_vis, aes(x = cluster, fill = atm)) + geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) + theme_minimal() +
  labs(title = "Conditions atmosphériques", x = "Cluster", y = "Proportion")

p5 <- ggplot(df_vis, aes(x = cluster, fill = col)) + geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) + theme_minimal() +
  labs(title = "Type de collision", x = "Cluster", y = "Proportion")
p1
p2
p3
p4
p5
```

```{r}
set.seed(321)
library(cluster)
library(factoextra)
library(viridis)

mds_df <- cmdscale(dist_gower, k = 2) %>% as.data.frame()
colnames(mds_df) <- c("Dim1", "Dim2")

fviz_cluster(list(data = mds_df, cluster = res_k3$cluster),
             geom = "point", palette = viridis(3), ellipse.type = "convex",
             show.clust.cent = TRUE, ggtheme = theme_minimal(),
             main = "Visualisation des clusters") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggsave("../Figures/clusters_fviz_final.png", width = 10, height = 8, dpi = 300)
```

Les groupes peuvent se chevaucher car la visualisation 2D (via PCA) projette un espace multidimensionnel sur un simple plan. Cette réduction dimensionnelle entraîne une perte d'information, et des clusters bien séparés dans l'espace original peuvent alors apparaître mélangés sur le graphique.

## H. Inteprétation de chaque cluster

### Cluster 1 : Accidents Urbains sous Intempéries

Ce groupe se distingue quasi-exclusivement par des conditions météorologiques et de surface dégradées.

-   **Conditions :** Forte prédominance de la pluie (légère ou forte) associée à une surface mouillée.

-   **Localisation :** Majoritairement en agglomération.

-   **Véhicules impliqués :** Une surreprésentation notable des transports en commun, suggérant des problématiques de freinage ou d'adhérence impliquant des véhicules lourds dans un trafic dense.

-   **Cinétique :** Vitesse moyenne faible.

### Cluster 2 : Accidents de Flux Urbain

Il s'agit du segment le plus volumineux, représentant l'accidentologie "quotidienne" en ville par conditions normales.

-   **Configuration :** Exclusivement en agglomération, concentré sur les intersections (carrefours, giratoires) avec une prédominance de chocs latéraux (refus de priorité, changements de file).

-   **Usagers Vulnérables :** Forte implication des deux-roues (vélos et motos).

-   **Conditions :** Environnement favorable (plein jour, météo normale, surface sèche).

-   **Cinétique :** Vitesse moyenne la plus basse des trois groupes.

### Cluster 3 : Accidents Routiers à Haute Cinétique

Ce cluster regroupe les accidents hors agglomération, caractérisés par une dangerosité potentielle accrue due à la vitesse.

-   **Localisation :** Routes départementales et autoroutes (hors agglomération).

-   **Facteurs aggravants :** Surreprésentation des accidents nocturnes ("Nuit sans éclairage") et des tracés en courbe.

-   **Typologie :** Nombreuses pertes de contrôle (sorties de route sur accotement) ou chocs frontaux.

-   **Véhicules :** Implication majeure des véhicules légers et poids lourds ; absence quasi-totale d'usagers vulnérables (piétons/vélos).

-   **Cinétique :** Vitesse moyenne très élevée.

## I. Conclusion

L'algorithme K-prototypes a identifié trois groupes d'accidents distincts en combinant variables numériques et catégorielles. Cette segmentation permet d'analyser les différents profils d'accidents et d'identifier les facteurs de risque spécifiques à chaque groupe.
