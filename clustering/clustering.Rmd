---
title: "clustering"
output: html_document
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Comme le jeu de données contient à la fois des variables numériques et des variables catégorielles (notamment la région, la condition d’éclairage, la condition atmosphérique, etc.), l'algorithme K-means classique n'est pas le plus adapté. En effet, K-means repose sur la distance euclidienne, qui perd son sens avec des variables catégorielles, même après un encodage one-hot.

Il est préférable d'utiliser K-prototypes. Cet algorithme combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité adaptée aux variables catégorielles. Il permet ainsi d'obtenir des clusters plus fiable et plus facile à interpréter.

## A. Chargement des données

Le jeu de données final étant très volumineux, un échantillon aléatoire a été extrait pour faciliter le traitement et le clustering.

```{r}
n_sample <- 1000
df_avec_var_rep <- read.csv('accidents_db_final_clusturing.csv')
df_avec_var_rep <- df_avec_var_rep[sample(nrow(df_avec_var_rep), n_sample), ]

df_sans_var_rep <- subset(df_avec_var_rep, select = -c(nb_tue, nb_hospitalise, nb_blesse, nb_indemne))
df_sans_var_exp <- df_sans_var_exp[sample(nrow(df_sans_var_exp), n_sample), ]
```

Identifier les variables de type character et les convertir en facteurs.

```{r}
variables_categorielles = sapply(df_avec_var_rep, is.character)

for (i in 1:length(variables_categorielles)) {
  if (variables_categorielles[i]) {
    df_avec_var_rep[[i]] = as.factor(df_avec_var_rep[[i]])
  }
}

variables_categorielles = sapply(df_sans_var_exp, is.character)

for (i in 1:length(df_sans_var_exp)) {
  if (variables_categorielles[i]) {
    df_sans_var_exp[[i]]= as.factor(df_sans_var_exp[[i]])
  }
}
```

Avant de passer à l'implémentation de l'algorithme de k-prototypes, il faut d'abord :

-   Vérifier que toutes les variables catégorielles sont bien des facteurs.

-   Vérifier que toutes les variable numériques sont bien des numériques.

-   Traiter les valeurs manquantes (NA) si nécessaire.

```{r}
str(df_avec_var_rep)
anyNA(df_avec_var_rep)

str(df_sans_var_exp)
```

## B. Normalisation des variables numériques

```{r}
numeric_cols <- sapply(df_avec_var_rep, is.numeric)
df_avec_var_rep[numeric_cols] <- scale(df_avec_var_rep[numeric_cols])

numeric_cols <- sapply(df_sans_var_rep, is.numeric)
df_sans_var_exp[numeric_cols] <- scale(df_sans_var_exp[numeric_cols])

```

*Remarque : les variables catégorielles (facteurs) ne sont pas standardisées.*

## C. Choix du nombre de clusters (k)

On applique l'algorithme k-prototypes pour différentes valeurs de k.

```{r}
set.seed(1)
#install.packages("clustMixType")
#install.packages("cluster")
library(clustMixType)
library(cluster)

# Pour la méthode de coude
cost <- numeric()

# Pour la méthode de silhouette
d <- daisy(df_avec_var_rep, metric = "gower")
sil_avg <- numeric()

for (k in 2:10) {
  res <- kproto(df_avec_var_rep, k)
  cost[k] <- res$tot.withinss
  
  sil <- silhouette(res$cluster, d)
  sil_avg[k] <- mean(sil[, 3])
}

plot(2:10, cost[2:k], main = "Méthode du coude pour choisir k",type = "b", xlab = "Nombre de clusters (k)", ylab = "Somme des distances")

plot(2:10, sil_avg[2:10], main = "Méthode de la silhouette pour choisir k",type = "b", xlab = "Nombre de clusters (k)", ylab = "Silhouette moyenne")
```

### Observations

Le graphique de la méthode du coude montre que la courbe se stabilise à **k = 4**, tandis que la silhouette présente deux maxima à **k = 2** et **k = 4**.\

Bien que le pic le plus élevé de la silhouette corresponde à k = 2, nous avons choisi **k = 4** car il est cohérent avec le coude et permet une segmentation plus détaillée des observations.

## D. Application de k-prototypes

```{r}
set.seed(1)
library(clustMixType)
res <- kproto(df_avec_var_rep, k = 4, diss = "gower")
clusters <- res$cluster
```

## E. Analyse des résultats

#### Distribution des observations par cluster

```{r}
table(clusters)
```

L'analyse des clusters montre que les quatres groupes identifiés présentent des tailles différentes. Le cluster 3 regroupe la majorité des observations, tandis que le cluster 1 correspond a un petit sous-groupe spécifique. Les clusters 2 et 4 occupent des tailles intermédiaires.

Cette répartition révèle l'existence de groupes homogènes mais de taille variées, permettant d'identifier et d'interpéter les profils distincts propres à chaque cluster.

### Moyennes des variables numériques par cluster

```{r}
aggregate(. ~ clusters, data = df_avec_var_rep, FUN = mean)
```

### **Répartition des variables catégorielles par cluster**

```{r}
cat_cols <- sapply(df_avec_var_rep, is.factor)

for (col in names(df_avec_var_rep)[cat_cols]) {
  cat("\nVariable :", col, "\n")
  print(table(df_avec_var_rep[[col]], clusters))
}
```

## F. Visualisation des clusters k-prototypes

```{r}
library(factoextra)
library(viridis)

# Extraire les clusters
clusters <- res$cluster

# Ne garder que les colonnes numériques
num_cols <- sapply(df_avec_var_rep, is.numeric)
df_num <- df_avec_var_rep[, num_cols]

fviz_cluster(list(data = df_num, cluster = clusters),
             geom = "point",
             palette = viridis(4), 
             ellipse.type = "convex",
             repel = TRUE,
             show.clust.cent = TRUE,
             ggtheme = theme_minimal(), 
             main = "Visualisation des clusters")
```

## B. Nettoyage des données

```{r}
#install.packages("dplyr")
library(dplyr)

variables_qualitatives = colnames(df_non_integer)
# On supprime les lignes avec NA dans les variables qualitatives
donnees_sans_na = df %>% na.omit(select(variables_qualitatives))

# On transforme mes qualitatives en factors
donnees_sans_na[, variables_qualitatives] = lapply(donnees_sans_na[, variables_qualitatives], as.factor)

# One-hot encoding avec model.matrix()
dummies = model.matrix(as.formula(paste("~", paste(variables_qualitatives, collapse = "+"), "-1")), data = donnees_sans_na)

data_final = cbind(donnees_sans_na[, setdiff(names(donnees_sans_na), variables_qualitatives)], dummies)

# Décommentez la ligne suivant si vous voulez générer le fichier csv
# write.csv(data_final, "../data/data_sans_na_one_hot_encoded.csv", row.names = FALSE)
```

Une fois que les données regroupées et nettoyées, nous pouvons commencer à implémenter l'algorithme des k-moyennes.

## C. Normalisation des données

```{r}
data_final = scale(data_final)

# Pour vérifier si les données sont bien normalisées 
#round(colMeans(data_final), 5)
#apply(data_final, 2, var)
```

## D. Implémentation de l'algorithme des k-moyennes

```{r}
#install.packages("factoextra")
library(factoextra)
echantillon = data_final[sample(nrow(data_final), 10000), ]
fviz_nbclust(echantillon, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
```

Le graphique représente l’inertie intra-cluster totale. Celle-ci diminue fortement jusqu’à k = 4, puis beaucoup plus lentement par la suite. Le coude est nettement visible à k = 4, ce qui indique que l’ajout de clusters supplémentaires n’apporte qu’un gain marginal.

Pour ces raisons, le nombre de clusters retenu est k = 4. Ce choix permet d’obtenir des groupes suffisamment distincts et interprétables tout en évitant une segmentation excessive des données.

## E. Visualisation des clusters k-means

Comme les données sont très nombreuses, il est difficile de voir clairement les clusters.

```{r}

#set.seed(123)
echantillon_clean <- echantillon[, apply(echantillon, 2, var) > 0]

#km.res = kmeans(echantillon_clean, 4, nstart = 25)
#fviz_cluster(
#  km.res,
#  data = echantillon_clean,
#  palette = c("blue", "green", "red", "lightblue"),
#  ellipse.type = "euclid",   # Confidence/concentration ellipse
#  star.plot = TRUE,          # Segments from centroid to points
#  repel = TRUE,              # Avoid overlapping labels
#  ggtheme = theme_min
```

## D. Implémentation de l'algorithme des k-Medoides

A l'inverse de la methode des K-Means les centroides font partie de la distribution. Cela permet de mieux prendre en considération les valeurs êxtremes.

### D.1 Recherche de la constante K

On utilise la méthode de silhouette qui permet d'estimer la qualité du clusturing à l'aide de la mesure du paramétre s(i) pour chaque i-éme point défini par :

$$
s(i)= \frac{a_i-b_i}{max(a_i,b_i)}
$$

ou • ai : est la moyenne de la distance de x_i au reste de points de sa catégorie

• b_i : ai : est la moyenne de la distance de x_i aux points de la catégorie la plus proche de x_i

```{r}
library(cluster)
library(factoextra)
fviz_nbclust(df_final, pam, method = "silhouette")+
theme_classic()
km.res = kmeans(echantillon_clean, 4, nstart = 25)
fviz_cluster(
  km.res,
  data = echantillon_clean,
  palette = c("blue", "green", "red", "lightblue"),
  ellipse.type = "euclid",   # Confidence/concentration ellipse
  star.plot = TRUE,          # Segments from centroid to points
  repel = TRUE,              # Avoid overlapping labels
  ggtheme = theme_minimal()
)
```

## F. Implémentation de l'algorithme des k-medoids

```{r}
# On utilise data_final étant normalisé
# install.packages("cluster")
library(cluster)
library(factoextra)

fviz_nbclust(echantillon, pam, method = "silhouette")+theme_classic()
```

## G. CLARA - Clustering Large Applications

```{r}
library(cluster)
library(factoextra)
fviz_nbclust(echantillon, clara, method = "silhouette")+theme_classic()

```

Selon le graphique obtenu, le nombre optimal de groupes est 5. Dans la section suivante, on classifie donc les données en 5 groupes.

Le code R ci-dessous exécue l'algorithme PAM avec k = 5.

```{r}
clara.res = clara(df, 5, samples = 50, pamLike = TRUE)
   
clara.res$medoids
```

```{r}
fviz_cluster(clara.res,
  palette = c("blue", "orange", "pink", "lightgreen", "brown"), # color palette
  ellipse.type = "t", # Concentration ellipse
  geom = "point", pointsize = 1,
  ggtheme = theme_classic()
  )
```

```{r}
# On applique l'algo de K-Medoids avec le k obtenu en haut :
k <- 
pam.res <- pam(df_final, k)
print(pam.res)
```

```{r}
# On ajoute le résultat du clusturing au df 
dd <- cbind(df_final, cluster = pam.res$cluster)
head(dd, n = 3)
```

On peut visualiser le résultat du clusturing à l'aide de la réduction de dimensions :
