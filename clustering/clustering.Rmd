---
title: "clustering"
output: html_document
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A. Charger, importer, observer un tableau de données

```{r}
df = read.csv('donnees_regroupees.csv')
classes_colonne = sapply(df, class)
df_non_integer = df[, classes_colonne != "integer"]
#colnames(df_non_integer)
```

## B. Nettoyage des données

```{r}
#install.packages("dplyr")
library(dplyr)

variables_qualitatives = colnames(df_non_integer)
# On supprime les lignes avec NA dans les variables qualitatives
donnees_sans_na = df %>% na.omit(select(variables_qualitatives))

# On transforme mes qualitatives en factors
donnees_sans_na[, variables_qualitatives] = lapply(donnees_sans_na[, variables_qualitatives], as.factor)

# One-hot encoding avec model.matrix()
dummies = model.matrix(as.formula(paste("~", paste(variables_qualitatives, collapse = "+"), "-1")), data = donnees_sans_na)

data_final = cbind(donnees_sans_na[, setdiff(names(donnees_sans_na), variables_qualitatives)], dummies)

# Décommentez la ligne suivant si vous voulez générer le fichier csv
# write.csv(data_final, "../data/data_sans_na_one_hot_encoded.csv", row.names = FALSE)
```

Une fois que les données regroupées et nettoyées, nous pouvons commencer à implémenter l'algorithme des k-moyennes.

## C. Normalisation des données

```{r}
data_final = scale(data_final)

# Pour vérifier si les données sont bien normalisées 
#round(colMeans(data_final), 5)
#apply(data_final, 2, var)
```

## D. Implémentation de l'algorithme des k-moyennes

```{r}
#install.packages("factoextra")
library(factoextra)
echantillon = data_final[sample(nrow(data_final), 15000), ]
fviz_nbclust(echantillon, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
```

Le graphique représente l’inertie intra-cluster totale. Celle-ci diminue fortement jusqu’à k = 4, puis beaucoup plus lentement par la suite.
Le coude est nettement visible à k = 4, ce qui indique que l’ajout de clusters supplémentaires n’apporte qu’un gain marginal.

Pour ces raisons, le nombre de clusters retenu est k = 4. Ce choix permet d’obtenir des groupes suffisamment distincts et interprétables tout en évitant une segmentation excessive des données.

## Visualisation des clusters k-means

Comme les données sont très nombreuses, il est difficile de voir clairement les clusters.

```{r}

#set.seed(123)
#echantillon_clean <- echantillon[, apply(echantillon, 2, var) > 0]

#km.res = kmeans(echantillon_clean, 4, nstart = 25)
#fviz_cluster(
#  km.res,
#  data = echantillon_clean,
#  palette = c("blue", "green", "red", "lightblue"),
#  ellipse.type = "euclid",   # Confidence/concentration ellipse
#  star.plot = TRUE,          # Segments from centroid to points
#  repel = TRUE,              # Avoid overlapping labels
#  ggtheme = theme_minimal()
#)


```

```         
```
