---
title: "clustering"
output: html_document
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A. Charger, importer, observer un tableau de données

```{r}
df = read.csv('donnees_regroupees.csv')
classes_colonne = sapply(df, class)
df_non_integer = df[, classes_colonne != "integer"]
#colnames(df_non_integer)
```

## B. Nettoyage des données

```{r}
#install.packages("dplyr")
library(dplyr)

variables_qualitatives = colnames(df_non_integer)
# On supprime les lignes avec NA dans les variables qualitatives
donnees_sans_na = df %>% na.omit(select(variables_qualitatives))

# On transforme mes qualitatives en factors
donnees_sans_na[, variables_qualitatives] = lapply(donnees_sans_na[, variables_qualitatives], as.factor)

# One-hot encoding avec model.matrix()
dummies = model.matrix(as.formula(paste("~", paste(variables_qualitatives, collapse = "+"), "-1")), data = donnees_sans_na)

data_final = cbind(donnees_sans_na[, setdiff(names(donnees_sans_na), variables_qualitatives)], dummies)

# Décommentez la ligne suivant si vous voulez générer le fichier csv
# write.csv(data_final, "../data/data_sans_na_one_hot_encoded.csv", row.names = FALSE)
```

Une fois que les données regroupées et nettoyées, nous pouvons commencer à implémenter l'algorithme des k-moyennes.

## C. Normalisation des données

```{r}
data_final = scale(data_final)

# Pour vérifier si les données sont bien normalisées 
#round(colMeans(data_final), 5)
#apply(data_final, 2, var)
```

## D. Implémentation de l'algorithme des k-moyennes

```{r}
#install.packages("factoextra")
library(factoextra)
echantillon = data_final[sample(nrow(data_final), 15000), ]
fviz_nbclust(echantillon, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
```

Le graphique représente l’inertie intra-cluster totale. Celle-ci diminue fortement jusqu’à k = 4, puis beaucoup plus lentement par la suite. Le coude est nettement visible à k = 4, ce qui indique que l’ajout de clusters supplémentaires n’apporte qu’un gain marginal.

Pour ces raisons, le nombre de clusters retenu est k = 4. Ce choix permet d’obtenir des groupes suffisamment distincts et interprétables tout en évitant une segmentation excessive des données.

## Visualisation des clusters k-means

Comme les données sont très nombreuses, il est difficile de voir clairement les clusters.

```{r}

#set.seed(123)
#echantillon_clean <- echantillon[, apply(echantillon, 2, var) > 0]

#km.res = kmeans(echantillon_clean, 4, nstart = 25)
#fviz_cluster(
#  km.res,
#  data = echantillon_clean,
#  palette = c("blue", "green", "red", "lightblue"),
#  ellipse.type = "euclid",   # Confidence/concentration ellipse
#  star.plot = TRUE,          # Segments from centroid to points
#  repel = TRUE,              # Avoid overlapping labels
#  ggtheme = theme_min
```

## D. Implémentation de l'algorithme des k-Medoides

A l'inverse de la methode des K-Means les centroides font partie de la distribution. Cela permet de mieux prendre en considération les valeurs êxtremes.

### D.1 Recherche de la constante K

On utilise la méthode de silhouette qui permet d'estimer la qualité du clusturing à l'aide de la mesure du paramétre s(i) pour chaque i-éme point défini par :

$$
s(i)= \frac{a_i-b_i}{max(a_i,b_i)}
$$

ou • ai : est la moyenne de la distance de x_i au reste de points de sa catégorie

• b_i : ai : est la moyenne de la distance de x_i aux points de la catégorie la plus proche de x_i

```{r}
library(cluster)
library(factoextra)
fviz_nbclust(df_final, pam, method = "silhouette")+
theme_classic()
```

```{r}
# On applique l'algo de K-Medoids avec le k obtenu en haut :
k <- 
pam.res <- pam(df_final, k)
print(pam.res)
```

```{r}
# On ajoute le résultat du clusturing au df 
dd <- cbind(df_final, cluster = pam.res$cluster)
head(dd, n = 3)
```

On peut visualiser le résultat du clusturing à l'aide de la réduction de dimensions :
