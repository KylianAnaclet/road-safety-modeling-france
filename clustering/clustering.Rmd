---
title: "Clustering K-prototypes"
subtitle: "Équipe 19 – ANACLET Kylian, BIN DAIVIN Muhamad Zaiinizee, CHENNOUF Younes, KEBAIRI Ilyes"
output:
  pdf_document: default
  html_document: default
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Le jeu de données contient à la fois des variables numériques et catégorielles (région, condition d'éclairage, condition atmosphérique, etc.). L'algorithme K-means classique n'est donc pas adapté, car il repose sur la distance euclidienne, qui n'a pas de sens pour les variables catégorielles, même après encodage one-hot.

Il est préférable d'utiliser **K-prototypes**, qui combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité spécifique pour les variables catégorielles. Cet algorithme permet ainsi d'obtenir des clusters plus fiables et cohérents.

# K-prototypes

L'algorithme K-prototypes implémente la méthode de partitionnement par minimisation de la distance entre les observations et les centres de clusters. Plus précisément, il cherche à minimiser la fonction suivante :

$$
\min_{c_1,...,c_k} \sum_{k=1}^{K} \sum_{i,G(i)=k} d(x^{(i)}, c_k)
$$

où $d(x^{(i)}, c_k)$ représente la distance entre l'observation $x^{(i)}$ et le centre du cluster $c_k$. Cette distance combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité pour les variables catégorielles.

La minimisation est réalisée automatiquement par la fonction `kproto()` à travers un processus itératif. L'algorithme affecte chaque observation au cluster dont le centre est le plus proche, puis recalcule les centres, et répète ces étapes jusqu'à convergence. Le résultat de cette minimisation est observable via `tot.withinss`, qui représente la somme totale des distances intra-clusters.

## A. Chargement et préparation des données

Le jeu de données final étant **très volumineux**, un échantillon aléatoire a été extrait pour faciliter le traitement et le clustering.

```{r}
n_sample <- 1000

# Exécuter le script « nettoyage_donnees_clustering.Rmd », situé dans le dossier clustering, afin de générer le fichier « donnees_clustering.csv ».
df <- read.csv("../data/donnees_clustering.csv")

# On enlève les variables non utiles pour le clustering (décommentez une des lignes suivantes)
#df <- subset(df, select = -c(Region, Moment))
df <- subset(df, select = -c(Region, Moment, nb_tue, nb_hospitalise, nb_blesse, nb_indemne))

# On tire par hasard 1000 accidents
df <- df[sample(nrow(df), n_sample), ]
```

On n’inclut pas ces variables :

-   **Region :** Son inclusion forcerait les clusters à se former principalement par région plutôt que par les caractéristiques réelles des accidents.

-   **Moment :** Cette variable est redondante avec `lum` (conditions d'éclairage).

-   **nb_tue, nb_hospitalise, nb_blesse, nb_indemne :** On les exclut du clustering car ce sont des résultats (gravité de l’accident).

Identifier les variables de type character et les convertir en facteurs.

```{r}
if(!require(dplyr)) install.packages("dplyr")
library(dplyr)

df <- df %>% mutate(across(where(is.character), as.factor))
```

Avant d'appliquer l'algorithme K-prototypes, il est nécessaire de :

-   Vérifier que toutes les variables catégorielles sont bien des facteurs.

-   Vérifier que toutes les variable numériques sont bien des numériques.

```{r}
str(df)
```

## B. Normalisation des variables numériques

Les variables numériques sont standardisées pour garantir que toutes contribuent équitablement au calcul des distances, indépendamment de leur échelle de mesure. Cela évite qu'une variable avec de grandes valeurs domine artificiellement la formation des clusters.

```{r}
numeric_cols <- sapply(df, is.numeric)
df_scaled <- df
df_scaled[numeric_cols] <- scale(df[numeric_cols])
```

*Remarque : les variables catégorielles (facteurs) ne sont pas standardisées.* \## C. Évaluation de la tendance à former des clusters :

Avant de commencer on doit tout d'abord s'assurer que l'utilisation de la méthode de clustering est légitime ici (s'assurer que les données ne sont pas réparties de façon homogène). Pour ce faire on utilise la méthode statistique en calculant la statistique de Hopkins calculé avec la formule :

$$
H = \frac{\sum_{i=1}^{n}{x_i}}{\sum_{i=1}^{n}{x_i}+\sum_{i=1}^{n}{y_i}}
$$

avec

-   **xi** : la distance d'un i éme point pi choisi aléatoirement dans notre distribution à tster avec son plus proche voisin dans la même distribution

-   **yi :** la distance d'un i éme point qi choisi aléatoirement dans une distribution aléatoire uniforme avec son plus proche voisin dans la distribution

Nos hypothèses étant :

-   **H0 :** Les données sont uniformément distribués

-   **H1 :** Les données ne sont pas uniformément distribués

    ```{r}
    library(hopkins)

    hopkins(df_scaled[, numeric_cols])
    ```

## C. Choix du nombre de clusters (k)

On applique l'algorithme k-prototypes pour différentes valeurs de k.

```{r}
set.seed(1)
if(!require(clustMixType)) install.packages("clustMixType")
if(!require(cluster)) install.packages("cluster")
library(clustMixType)
library(cluster)

k_max <- 10
cost <- numeric(k_max)
sil_avg <- numeric(k_max)

# Calculer la distance de Gower sur les données non normalisées
d <- daisy(df, metric = "gower")

# Exécuter k-prototypes pour différentes valeurs de k
for (k in 2:k_max) {
  kp <- kproto(df_scaled, k, verbose = FALSE)
  cost[k] <- kp$tot.withinss
  
  sil <- silhouette(kp$cluster, d)
  sil_avg[k] <- mean(sil[, 3])
}

# Plot méthode du coude
plot(2:k_max, cost[2:k_max], 
     main = "Méthode du coude pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Somme des distances")

# Plot méthode de la silhouette
plot(2:k_max, sil_avg[2:k_max], 
     main = "Méthode de la silhouette pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Silhouette moyenne")
```

### Observations

Le graphique de la méthode du coude montre que la courbe se stabilise à k = 6, tandis que la silhouette présente deux maxima à k = 2 et k = 4.

Bien que le pic le plus élevé de la silhouette corresponde à k = 2, nous avons choisi k = 4 car il est cohérent avec le coude et permet une segmentation plus détaillée des observations.

*Remarque : Ces résultats dépendent fortement de l'échantillon tiré. Avec un autre échantillon aléatoire, les courbes du coude et de la silhouette pourraient suggérer un nombre de clusters différent.*

## D. Application de k-prototypes et analyse des résultats

```{r}
set.seed(1)
library(clustMixType)

res <- kproto(df_scaled, k = 4, verbose = FALSE)
clusters <- res$cluster
```

#### Distribution des observations par cluster

```{r}
table(clusters)
```

L'analyse des clusters montre que les quatres groupes identifiés présentent des tailles différentes. Le cluster 3 regroupe la majorité des observations, tandis que le cluster 1 correspond a un petit sous-groupe spécifique. Les clusters 2 et 4 occupent des tailles intermédiaires.

Cette répartition révèle l'existence de groupes homogènes mais de taille variées, permettant d'identifier et d'interpéter les profils distincts propres à chaque cluster.

*Remarque : Ces effectifs varient selon l'échantillon sélectionné.*

### Moyennes des variables numériques par cluster

```{r}
# Sélectionner uniquement les colonnes numériques
numeric_df <- df[, sapply(df, is.numeric)]
numeric_df$clusters <- clusters

aggregate(. ~ clusters, data = numeric_df, FUN = mean)
```

### **Répartition des variables catégorielles par cluster**

```{r}
cat_cols <- sapply(df, is.factor)

for (col in names(df)[cat_cols]) {
  cat("\nVariable :", col, "\n")
  print(table(df[[col]], clusters))
}
```

## E. Visualisation des clusters k-prototypes

```{r}
if(!require(factoextra)) install.packages("factoextra")
if(!require(viridis)) install.packages("viridis")
library(factoextra)
library(viridis)

clusters <- res$cluster

num_cols <- sapply(df_scaled, is.numeric)
df_num <- df_scaled[, num_cols]

p <- fviz_cluster(list(data = df_num, cluster = clusters),
             geom = "point",
             palette = viridis(4), 
             ellipse.type = "convex",
             repel = TRUE,
             show.clust.cent = TRUE,
             ggtheme = theme_minimal(), 
             main = "Visualisation des clusters")
p
# Export the plot
ggsave(
  filename = "../Figures/clusters_visualisation.png",
  plot = p,
  width = 8,
  height = 6,
  dpi = 300
)
```

### Observations

Les groupes peuvent se chevaucher car la visualisation 2D (via PCA) projette un espace multidimensionnel sur un simple plan. Cette réduction dimensionnelle entraîne une perte d'information, et des clusters bien séparés dans l'espace original peuvent alors apparaître mélangés sur le graphique.

## F. Test de la qualité du clustering :

Afin d'évaluer la qualité du clustering on évalue les deux indices :

-   l'indice de Silhouette pour chaque cluster qui pour chaque élément évalue à quel point un élément est "similaire" par rapport aux autres éléments du même cluster .

-   l'indice de Dunn qui est défini comme suit :

    $$
    D=\frac{min.separation}{max.diameter}
    $$

    De plus on veille à adapter cette methode d'évaluation à la methode de clustering utilisée (k-prototype) et ce en utilisant justement la distance de gower qui prend en compte les variables catégorielles

```{r}
library(cluster)  # Pour daisy et silhouette
library(fpc)      # Pour cluster.stats (Dunn)


# Calcul de la matrice de distance de Gower 
dist_gower <- daisy(df, metric = "gower")

# On croise la distance réelle (Gower) avec les clusters existants
sil_obj <- silhouette(clusters, dist_gower)

# Affichage des résultats Silhouette
cat("Score de Silhouette Moyen :", summary(sil_obj)$avg.width, "\n")

stats_clusters <- cluster.stats(d = dist_gower, clustering = clusters)

cat("Indice de Dunn :", stats_clusters$dunn, "\n")

# On affiche l'indice silouhette pour chaque cluster pour identifier les clusters 'les mieux formés'
print("Détail par cluster (Silhouette) :")
summary(sil_obj)$clus.avg.widths
```

## E. Inteprétation de chaque cluster :

### Cluster 1 : Accidents Urbains sous Intempéries

Ce groupe se distingue quasi-exclusivement par des conditions météorologiques et de surface dégradées.

-   **Conditions :** Forte prédominance de la pluie (légère ou forte) associée à une surface mouillée.

-   **Localisation :** Majoritairement en agglomération.

-   **Véhicules impliqués :** Une surreprésentation notable des transports en commun, suggérant des problématiques de freinage ou d'adhérence impliquant des véhicules lourds dans un trafic dense.

-   **Cinétique :** Vitesse moyenne faible.

### Cluster 2 : Accidents de Flux Urbain 

Il s'agit du segment le plus volumineux, représentant l'accidentologie "quotidienne" en ville par conditions normales.

-   **Configuration :** Exclusivement en agglomération, concentré sur les intersections (carrefours, giratoires) avec une prédominance de chocs latéraux (refus de priorité, changements de file).

-   **Usagers Vulnérables :** Forte implication des deux-roues (vélos et motos).

-   **Conditions :** Environnement favorable (plein jour, météo normale, surface sèche).

-   **Cinétique :** Vitesse moyenne la plus basse des trois groupes.

### Cluster 3 : Accidents Routiers à Haute Cinétique

Ce cluster regroupe les accidents hors agglomération, caractérisés par une dangerosité potentielle accrue due à la vitesse.

-   **Localisation :** Routes départementales et autoroutes (hors agglomération).

-   **Facteurs aggravants :** Surreprésentation des accidents nocturnes ("Nuit sans éclairage") et des tracés en courbe.

-   **Typologie :** Nombreuses pertes de contrôle (sorties de route sur accotement) ou chocs frontaux.

-   **Véhicules :** Implication majeure des véhicules légers et poids lourds ; absence quasi-totale d'usagers vulnérables (piétons/vélos).

-   **Cinétique :** Vitesse moyenne très élevée.

## G. Conclusion

L'algorithme K-prototypes a identifié quatre groupes d'accidents distincts en combinant variables numériques et catégorielles. Cette segmentation permet d'analyser les différents profils d'accidents et d'identifier les facteurs de risque spécifiques à chaque groupe.

Ces résultats dépendent de l'échantillon sélectionné et pourraient varier avec un échantillonnage différent.
