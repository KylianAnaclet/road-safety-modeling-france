---
title: "clustering"
output: html_document
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A. Préparation des données

```{r}
df <- read.csv('accidents_db_final_clusturing.csv') 
df <- na.omit(df)

```

Identifier les variables de type character et les convertir en facteurs.

```{r}
data_fac <- df
char_cols = sapply(df, is.character)

for (i in 1:length(char_cols)) {
  if (char_cols[i]) {
    data_fac[[i]] <- as.factor(data[[i]])
  }
}
```

Comme le jeu de données contient à la fois des variables numériques et des variables catégorielles (notamment la région, la condition d’éclairage, la condition atmosphérique, etc.), l'algorithme K-means classique n'est pas le plus adapté. En effet, K-means repose sur la distance euclidienne, qui perd son sens avec des variables catégorielles, même après un encodage one-hot.

Il est préférable d'utiliser K-prototypes. Cet algorithme combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité adaptée aux variables catégorielles. Il permet ainsi d'obtenir des clusters plus fiable et plus facile à interpréter.

## B. Nettoyage des données

```{r}
#install.packages("dplyr")
library(dplyr)

variables_qualitatives = colnames(df_non_integer)
# On supprime les lignes avec NA dans les variables qualitatives
donnees_sans_na = df %>% na.omit(select(variables_qualitatives))

# On transforme mes qualitatives en factors
donnees_sans_na[, variables_qualitatives] = lapply(donnees_sans_na[, variables_qualitatives], as.factor)

# One-hot encoding avec model.matrix()
dummies = model.matrix(as.formula(paste("~", paste(variables_qualitatives, collapse = "+"), "-1")), data = donnees_sans_na)

data_final = cbind(donnees_sans_na[, setdiff(names(donnees_sans_na), variables_qualitatives)], dummies)

# Décommentez la ligne suivant si vous voulez générer le fichier csv
# write.csv(data_final, "../data/data_sans_na_one_hot_encoded.csv", row.names = FALSE)
```

Une fois que les données regroupées et nettoyées, nous pouvons commencer à implémenter l'algorithme des k-moyennes.

## C. Normalisation des données

```{r}
data_final = scale(data_final)

# Pour vérifier si les données sont bien normalisées 
#round(colMeans(data_final), 5)
#apply(data_final, 2, var)
```

## D. Implémentation de l'algorithme des k-moyennes

```{r}
#install.packages("factoextra")
library(factoextra)
echantillon = data_final[sample(nrow(data_final), 10000), ]
fviz_nbclust(echantillon, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
```

Le graphique représente l’inertie intra-cluster totale. Celle-ci diminue fortement jusqu’à k = 4, puis beaucoup plus lentement par la suite. Le coude est nettement visible à k = 4, ce qui indique que l’ajout de clusters supplémentaires n’apporte qu’un gain marginal.

Pour ces raisons, le nombre de clusters retenu est k = 4. Ce choix permet d’obtenir des groupes suffisamment distincts et interprétables tout en évitant une segmentation excessive des données.

## E. Visualisation des clusters k-means

Comme les données sont très nombreuses, il est difficile de voir clairement les clusters.

```{r}

#set.seed(123)
echantillon_clean <- echantillon[, apply(echantillon, 2, var) > 0]

#km.res = kmeans(echantillon_clean, 4, nstart = 25)
#fviz_cluster(
#  km.res,
#  data = echantillon_clean,
#  palette = c("blue", "green", "red", "lightblue"),
#  ellipse.type = "euclid",   # Confidence/concentration ellipse
#  star.plot = TRUE,          # Segments from centroid to points
#  repel = TRUE,              # Avoid overlapping labels
#  ggtheme = theme_min
```

## D. Implémentation de l'algorithme des k-Medoides

A l'inverse de la methode des K-Means les centroides font partie de la distribution. Cela permet de mieux prendre en considération les valeurs êxtremes.

### D.1 Recherche de la constante K

On utilise la méthode de silhouette qui permet d'estimer la qualité du clusturing à l'aide de la mesure du paramétre s(i) pour chaque i-éme point défini par :

$$
s(i)= \frac{a_i-b_i}{max(a_i,b_i)}
$$

ou • ai : est la moyenne de la distance de x_i au reste de points de sa catégorie

• b_i : ai : est la moyenne de la distance de x_i aux points de la catégorie la plus proche de x_i

```{r}
library(cluster)
library(factoextra)
fviz_nbclust(df_final, pam, method = "silhouette")+
theme_classic()
km.res = kmeans(echantillon_clean, 4, nstart = 25)
fviz_cluster(
  km.res,
  data = echantillon_clean,
  palette = c("blue", "green", "red", "lightblue"),
  ellipse.type = "euclid",   # Confidence/concentration ellipse
  star.plot = TRUE,          # Segments from centroid to points
  repel = TRUE,              # Avoid overlapping labels
  ggtheme = theme_minimal()
)
```

## F. Implémentation de l'algorithme des k-medoids

```{r}
# On utilise data_final étant normalisé
# install.packages("cluster")
library(cluster)
library(factoextra)

fviz_nbclust(echantillon, pam, method = "silhouette")+theme_classic()
```

## G. CLARA - Clustering Large Applications

```{r}
library(cluster)
library(factoextra)
fviz_nbclust(echantillon, clara, method = "silhouette")+theme_classic()

```

Selon le graphique obtenu, le nombre optimal de groupes est 5. Dans la section suivante, on classifie donc les données en 5 groupes.

Le code R ci-dessous exécue l'algorithme PAM avec k = 5.

```{r}
clara.res = clara(df, 5, samples = 50, pamLike = TRUE)
   
clara.res$medoids
```

```{r}
fviz_cluster(clara.res,
  palette = c("blue", "orange", "pink", "lightgreen", "brown"), # color palette
  ellipse.type = "t", # Concentration ellipse
  geom = "point", pointsize = 1,
  ggtheme = theme_classic()
  )
```

```{r}
# On applique l'algo de K-Medoids avec le k obtenu en haut :
k <- 
pam.res <- pam(df_final, k)
print(pam.res)
```

```{r}
# On ajoute le résultat du clusturing au df 
dd <- cbind(df_final, cluster = pam.res$cluster)
head(dd, n = 3)
```

On peut visualiser le résultat du clusturing à l'aide de la réduction de dimensions :
