---
title: "Clustering K-prototypes"
subtitle: "Équipe 19 – ANACLET Kylian, BIN DAIVIN Muhamad Zaiinizee, CHENNOUF Younes, KEBAIRI Ilyes"
output:
  pdf_document: default
  html_document: default
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Le jeu de données contient à la fois des variables numériques et catégorielles (région, condition d'éclairage, condition atmosphérique, etc.). L'algorithme K-means classique n'est donc pas adapté, car il repose sur la distance euclidienne, qui n'a pas de sens pour les variables catégorielles, même après encodage one-hot.

Il est préférable d'utiliser **K-prototypes**, qui combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité spécifique pour les variables catégorielles. Cet algorithme permet ainsi d'obtenir des clusters plus fiables et cohérents.

# K-prototypes

L'algorithme K-prototypes implémente la méthode de partitionnement par minimisation de la distance entre les observations et les centres de clusters. Plus précisément, il cherche à minimiser la fonction suivante :

$$
\min_{c_1,...,c_k} \sum_{k=1}^{K} \sum_{i,G(i)=k} d(x^{(i)}, c_k)
$$

où $d(x^{(i)}, c_k)$ représente la distance entre l'observation $x^{(i)}$ et le centre du cluster $c_k$. Cette distance combine la distance euclidienne pour les variables numériques et une mesure de dissimilarité pour les variables catégorielles.

La minimisation est réalisée automatiquement par la fonction `kproto()` à travers un processus itératif. L'algorithme affecte chaque observation au cluster dont le centre est le plus proche, puis recalcule les centres, et répète ces étapes jusqu'à convergence. Le résultat de cette minimisation est observable via `tot.withinss`, qui représente la somme totale des distances intra-clusters.

## A. Chargement et préparation des données

Le jeu de données final étant **très volumineux**, un échantillon aléatoire a été extrait pour faciliter le traitement et le clustering.

```{r}
n_sample <- 1000

# Exécuter le script « nettoyage_donnees_clustering.Rmd », situé dans le dossier clustering, afin de générer le fichier « donnees_clustering.csv ».
df <- read.csv("../data/donnees_clustering.csv")

# On enlève les variables non utiles pour le clustering (décommentez une des lignes suivantes)
#df <- subset(df, select = -c(Region, Moment))
#df <- subset(df, select = -c(Region, Moment, nb_tue, nb_hospitalise, nb_blesse, nb_indemne))

# On tire par hasard 1000 accidents
df <- df[sample(nrow(df), n_sample), ]
```

On n’inclut pas ces variables :

-   **Region :** Son inclusion forcerait les clusters à se former principalement par région plutôt que par les caractéristiques réelles des accidents.

-   **Moment :** Cette variable est redondante avec `lum` (conditions d'éclairage).

-   **nb_tue, nb_hospitalise, nb_blesse, nb_indemne :** On les exclut du clustering car ce sont des résultats (gravité de l’accident).

Identifier les variables de type character et les convertir en facteurs.

```{r}
if(!require(dplyr)) install.packages("dplyr")
library(dplyr)

df <- df %>% mutate(across(where(is.character), as.factor))
```

Avant d'appliquer l'algorithme K-prototypes, il est nécessaire de :

-   Vérifier que toutes les variables catégorielles sont bien des facteurs.

-   Vérifier que toutes les variable numériques sont bien des numériques.

```{r}
str(df)
```

## B. Normalisation des variables numériques

Les variables numériques sont standardisées pour garantir que toutes contribuent équitablement au calcul des distances, indépendamment de leur échelle de mesure. Cela évite qu'une variable avec de grandes valeurs domine artificiellement la formation des clusters.

```{r}
numeric_cols <- sapply(df, is.numeric)
df_scaled <- df
df_scaled[numeric_cols] <- scale(df[numeric_cols])
```

*Remarque : les variables catégorielles (facteurs) ne sont pas standardisées.*

## C. Choix du nombre de clusters (k)

On applique l'algorithme k-prototypes pour différentes valeurs de k.

```{r}
set.seed(1)
if(!require(clustMixType)) install.packages("clustMixType")
if(!require(cluster)) install.packages("cluster")
library(clustMixType)
library(cluster)

k_max <- 10
cost <- numeric(k_max)
sil_avg <- numeric(k_max)

# Calculer la distance de Gower sur les données non normalisées
d <- daisy(df, metric = "gower")

# Exécuter k-prototypes pour différentes valeurs de k
for (k in 2:k_max) {
  kp <- kproto(df_scaled, k, verbose = FALSE)
  cost[k] <- kp$tot.withinss
  
  sil <- silhouette(kp$cluster, d)
  sil_avg[k] <- mean(sil[, 3])
}

# Plot méthode du coude
plot(2:k_max, cost[2:k_max], 
     main = "Méthode du coude pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Somme des distances")

# Plot méthode de la silhouette
plot(2:k_max, sil_avg[2:k_max], 
     main = "Méthode de la silhouette pour choisir k",
     type = "b", xlab = "Nombre de clusters (k)", 
     ylab = "Silhouette moyenne")
```

### Observations

Le graphique de la méthode du coude montre que la courbe se stabilise à k = 6, tandis que la silhouette présente deux maxima à k = 2 et k = 4.

Bien que le pic le plus élevé de la silhouette corresponde à k = 2, nous avons choisi k = 4 car il est cohérent avec le coude et permet une segmentation plus détaillée des observations.

*Remarque : Ces résultats dépendent fortement de l'échantillon tiré. Avec un autre échantillon aléatoire, les courbes du coude et de la silhouette pourraient suggérer un nombre de clusters différent.*

## D. Application de k-prototypes et analyse des résultats

```{r}
set.seed(1)
library(clustMixType)
res <- kproto(df_scaled, k = 4, verbose = FALSE)
clusters <- res$cluster
```

#### Distribution des observations par cluster

```{r}
table(clusters)
```

L'analyse des clusters montre que les quatres groupes identifiés présentent des tailles différentes. Le cluster 3 regroupe la majorité des observations, tandis que le cluster 1 correspond a un petit sous-groupe spécifique. Les clusters 2 et 4 occupent des tailles intermédiaires.

Cette répartition révèle l'existence de groupes homogènes mais de taille variées, permettant d'identifier et d'interpéter les profils distincts propres à chaque cluster.

*Remarque : Ces effectifs varient selon l'échantillon sélectionné.*

### Moyennes des variables numériques par cluster

```{r}
# Sélectionner uniquement les colonnes numériques
numeric_df <- df[, sapply(df, is.numeric)]
numeric_df$clusters <- clusters

aggregate(. ~ clusters, data = numeric_df, FUN = mean)
```

### **Répartition des variables catégorielles par cluster**

```{r}
cat_cols <- sapply(df, is.factor)

for (col in names(df)[cat_cols]) {
  cat("\nVariable :", col, "\n")
  print(table(df[[col]], clusters))
}
```

## E. Visualisation des clusters k-prototypes

```{r}
if(!require(factoextra)) install.packages("factoextra")
if(!require(viridis)) install.packages("viridis")
library(factoextra)
library(viridis)

clusters <- res$cluster

num_cols <- sapply(df_scaled, is.numeric)
df_num <- df_scaled[, num_cols]

fviz_cluster(list(data = df_num, cluster = clusters),
             geom = "point",
             palette = viridis(4), 
             ellipse.type = "convex",
             repel = TRUE,
             show.clust.cent = TRUE,
             ggtheme = theme_minimal(), 
             main = "Visualisation des clusters")
```

### Observations

Les groupes peuvent se chevaucher car la visualisation 2D (via PCA) projette un espace multidimensionnel sur un simple plan. Cette réduction dimensionnelle entraîne une perte d'information, et des clusters bien séparés dans l'espace original peuvent alors apparaître mélangés sur le graphique.

## F. Conclusion

L'algorithme K-prototypes a identifié quatre groupes d'accidents distincts en combinant variables numériques et catégorielles. Cette segmentation permet d'analyser les différents profils d'accidents et d'identifier les facteurs de risque spécifiques à chaque groupe.

Ces résultats dépendent de l'échantillon sélectionné et pourraient varier avec un échantillonnage différent.
