---
title: "fréquence"
output:
  pdf_document: default
  html_document: default
date: "2025-12-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1) Chargement et préparation des données

```{r}
df_accidents <- read.csv("../data/ONISR-2021.csv")
```

```{r}
library(dplyr)
df_frequence <- df_accidents %>%
  group_by(dep) %>%  # On regroupe tout par département
  summarise(
    #NOTRE VARIABLE Y (réponse)
    Nb_Accidents = n(),
    #Variables explicatives X
   
    Part_Pluie = mean(atm == "pluie forte" | atm == "pluie legere", na.rm = TRUE),
    
    Part_Autoroute = mean(catr == "autoroute", na.rm = TRUE),
    
    Part_Departementale = mean(catr == "departementale", na.rm = TRUE),
  
    
    Part_Hors_Agglo = mean(agg == "hors agglo", na.rm = TRUE),

    Part_Virage = mean(plan != "rectiligne", na.rm = TRUE),
  

    VMA_Moyenne = mean(as.numeric(vma), na.rm = TRUE),

    Taux_Mortalite = mean(nb_tue > 0, na.rm = TRUE)
  )
head(df_frequence)

```

## 2) Le CSV pour l'exposition : Population

On a pris les données provenant de l'INSEE 2021 via cette adresse : <https://www.insee.fr/fr/statistiques/7739582?sommaire=7728826>

```{r}
df_pop_brut <- read.csv("donnees_departements_INSEE_2021.csv", sep = ";", stringsAsFactors = FALSE)

#On nettoie les données
df_pop <- df_pop_brut %>%
  dplyr::select(DEP, PTOT) %>%       
  rename(dep = DEP, Population = PTOT) %>% 
  mutate(dep = as.character(dep))     
#On conserve uniquement la population et le code département, 
#que l'on renomme et convertit en texte pour permettre la jointure.
```

## 3) Fusion des deux CSV

On crée un nouveau CSV qui contient toutes les données de base plus une colonne Population

```{r}
df_final <- inner_join(df_frequence, df_pop, by = "dep")
```

## 4) Tests : Modèle linéaire généarlisé

Premièrement, on essaye avec une Loi de poisson parce que La loi de Poisson est pertinente pour décrire le nombre d'événements dans d'autres types d'intervalles, spatiaux plutôt que temporels, comme des segments, surfaces ou volumes.

Nb_Accidents agit comme un "compteur"

De plus, on prend en compte l'exposition, d'après une formule qu'on a trouvé sur <https://perso.univ-rennes1.fr/valerie.monbet/GLM/GLMpharma.pdf>, il mentionne notamment (diapo 146/203) la présence d'un offset qui dans notre cas est la population (il nous dit exactement que : "*L’offset est fréquent dans les modèles log-linéaires. Il représente souvent le log d’une mesure d’exposition*"

(Cela explique pourquoi on a rajouté un CSV avec les données de la population en 2021 de l'INSEE

(*PS: le pdf mis ci-dessus nous a été très utile notamment toute la partie sur Régression de Poisson, dans lequel on retrouve notamment des propriétés notamment le calcule de la déviance permettant de mesurer l'écart entre le modèle et l'observation*)

### 4.1) Test pour une loi de Poisson

Avant toute chose, pour utiliser la régression de Poisson :

L’offset permet de modéliser un **taux d’accidents par habitant**, et non un nombre brut. Sans offset, les départements très peuplés auraient automatiquement plus d’accidents, ce qui biaiserait complètement le modèle

```{r}
# --- LE MODÈLE NUL (Référence) ---
#Il ne prend en compte que l'Offset de la population
modele_nul <- glm(Nb_Accidents ~ 1 + offset(log(Population)), 
                  family = poisson, 
                  data = df_final)
modele_nul
```

CI-dessous, notre modèle avec les variables explicatives

```{r}
#Voici notre modèle
modele_poisson <- glm(Nb_Accidents ~ Part_Pluie + Part_Autoroute + 
                      Part_Departementale + Part_Hors_Agglo + 
                      Part_Virage + VMA_Moyenne +
                      offset(log(Population)), 
                      family = poisson, 
                      data = df_final)
modele_poisson
summary(modele_poisson)
```

$$
D(y, \hat{\mu}) = 2 \sum_{i=1}^n \left[ y_i \ln\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i) \right]
$$

Voici la formule de la Déviance, où

\- $y_i$ est le nb d'accidents **observé** (Réel) dans le département $i$

\-$\hat{\mu}$ est le nb d'accident **estimé** (Prédit) par le modèle pour un département $i$

\-$n$ est le nb total d'observations

De plus, si on regarde le summary =\> On va trouver notamment le AIC : C'est un score qui arbitre entre la précision du modèle et sa simplicité, en pénalisant l'ajout de variables pour éviter le surapprentissage.

```{r}
Deviance <- deviance(modele_poisson)

#Degré de liberté
Df <- df.residual(modele_poisson)

#Superposition

Phi <- Deviance / Df

round(Deviance,2)
round(Phi,2)
```

```{r}
print(paste("Déviance du Modèle Nul :", round(deviance(modele_nul), 2)))
print(paste("Déviance de notre Modèle   :", round(deviance(modele_poisson), 2)))
```

Ou bien on peut raisonner de la façon suivante :

on pose $H_0$ : le modèle permet de reproduire les obs ;

$H_1$ le modèle ne permet pas de reproduire les observations

On peut utiliser la statistique de Pearson qui vérifie une loi du Chi2 à n-p degré de liberté

```{r}
# 1. Calcul de la statistique de Pearson (
res_pearson <- sum(residuals(modele_poisson, type = "pearson")^2)

# 2. Degrés de liberté 
ddl <- df.residual(modele_poisson)

p_value <- pchisq(res_pearson, df = ddl, lower.tail = FALSE)

print(paste("Statistique de Pearson :", round(res_pearson, 2)))
```

Ici, un apercu graphique de notre modèle de Poisson par rapport à la réalité:

```{r}
library(ggplot2)
library(ggrepel) # Pour afficher les noms des départements sans chevauchement


df_final$Prediction_Poisson <- fitted(modele_poisson)

# 2. Le Graphe
ggplot(df_final, aes(x = Prediction_Poisson, y = Nb_Accidents)) +
  geom_point(alpha = 0.6, color = "blue") +
  
  # La ligne de perfection (y = x)
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed", size = 1) +
  
  geom_text_repel(aes(label = ifelse(abs(Nb_Accidents - Prediction_Poisson) > 200, dep, "")),
                  box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') +
  
  labs(title = "Diagnostic Poisson : Prédictions vs Réalité",
       x = "Nb Accident PRÉDIT (Modèle)",
       y = "Nb Accident RÉEL (Observé)") +
  theme_minimal()
```

```{r}
library(dplyr)
```

```{r}

#Visuel des résidus
df_final$Residus_Pearson <- residuals(modele_poisson, type = "pearson")

ggplot(df_final, aes(x = Prediction_Poisson, y = Residus_Pearson)) +
  geom_point(alpha = 0.6) +
  
  # Lignes de tolérance théorique (-2 et +2 écarts-types)
  geom_hline(yintercept = c(-2, 2), color = "red", linetype = "dashed") +
  geom_hline(yintercept = 0, color = "blue") +
  
  # Si on voit des points très loin (ex: > 5 ou < -5), c'est de la surdispersion
  labs(title = "Preuve de la Surdispersion (Structure en Entonnoir)",
       subtitle = "La variance augmente avec la moyenne (les points s'écartent vers la droite)",
       x = "Nombre d'accidents Prédit",
       y = "Résidus Standardisés (Pearson)") +
  theme_minimal()
```

Ainsi, on voit que le modèle de Poisson ne prédit pas correctement la fréquence des accidents la défiance étant très élevé le Chi-test étant pas en adéquation avec ce que l'on recherche.

On a donc décidé de continuer la démarche établi par le PDF en utilisant un modèle de Régression binomiale négative. En effet, le modèle binomiale négative va surement permettre de réduire cet effet de surdispersion

### 4.2) Test pour modèle binomiale négative

```{r}
library(MASS) # Pour utiliser glm.nb()


modele_nb <- glm.nb(Nb_Accidents ~ Part_Pluie + Part_Autoroute + 
                    Part_Departementale + Part_Hors_Agglo + 
                    Part_Virage + VMA_Moyenne +
                    offset(log(Population)), 
                    data = df_final)

summary(modele_nb)


```

A présent, comparons nos deux modèles. On va comparer les AIC et extraire les coefficents du modèle le plus efficace.

```{r}

#On compare les AIC

aic_poisson <- AIC(modele_poisson)
aic_nb      <- AIC(modele_nb)

cat("AIC Poisson            :", round(aic_poisson, 2), "\n")
cat("AIC Binomiale Négative :", round(aic_nb, 2), "\n")
```

On voit que le modèle Binomiale Négative est plus intéressant car l'AIC est nettement plus petit

```{r}
coefficients_binomiaux_négative <- exp(coef(modele_nb))

print(round(coefficients_binomiaux_négative, 3))
```

AInsi, on peut interpréter cela de la façon suivante :

**Virage & Autoroute :** Facteurs critiques majeurs, ils **multiplient par 6** le nombre d'accidents.

**Hors-Agglo :** Le plus protecteur, il **divise par 4** le risque par rapport à la ville.

**Pluie :** Protecteur paradoxal (prudence accrue), elle **divise par 2** les accidents.

**Départementale :** Réduit modérément le risque (**-30%**).

**VMA :** Impact négligeable (**-2%**).

```{r}

theta <- modele_nb$theta

mu <- fitted(modele_nb)

# On estime la variance observée "brute" par le carré de l'erreur
var_obs <- (df_final$Nb_Accidents - mu)^2

data_comparaison <- data.frame(Moyenne = mu, Variance_Obs = var_obs)

# 3. Le Graphique du Duel
ggplot(data_comparaison, aes(x = Moyenne, y = Variance_Obs)) +
  # A. Les points réels (L'erreur observée)
  geom_point(alpha = 0.3, color = "grey40") +
  
  # Poisson
  geom_abline(intercept = 0, slope = 1, color = "blue", size = 1, linetype = "dashed") +
  
  # Binomiale Négative
  stat_function(fun = function(x) x + (x^2 / theta), 
                color = "red", size = 1.2) +
  
  labs(title = "Poisson vs Binomiale négative",
       subtitle = "En Bleu : Notre modèle de Poisson (Var = Moyenne\n En Rouge : Binomial Négatif (Var qui explose)\n .",
       x = "Moyenne Prédite (Espérance)",
       y = "Variance Estimée (Erreur²)") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, max(var_obs)*0.8)) # Uniquement pour zoomer pour rien oublier
```

Ainsi, graphiquement, on constate que cela a plus de sens car nos petits points gris (les erreurs de prédiction au carré pour chaque département) suivent plus la droite rouge que la droite bleu et donc c'est cohérent que le modèle Binomiale Négative
